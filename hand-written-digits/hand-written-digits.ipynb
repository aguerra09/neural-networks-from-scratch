{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "7bb263e99aee1401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Backpropagation algorithm\n",
    "<font size=\"3\">\n",
    "Feedforward (inferencia):\n",
    "$$W · x = y_{pred}$$\n",
    "\n",
    "Backpropagation algorithm (entrenamiento):\n",
    "1. Inicializo los pesos ($W$) de manera aleatoria.\n",
    "2. Obtengo una primera predicción (pobre ya que los pesos son aleatorios). Cuanto de \"pobre\" lo calcula la función de pérdida:$$\\displaystyle\\sum_{k=1}^n(y_{pred} - y_{train})^2$$\n",
    "3. En el entrenamiento, como $x$ (la entrada) e $y_{train}$ (la salida) son fijas, solo se pueden modificar los pesos ($W$) para ajustar la red neuronal.\n",
    "4. Para ajustar $W$ calculamos el gradiente de la función de pérdida ya que el gradiente describe la direción que tomaria $y_{pred}$ al aumentar cualquiera de los valores de $W$: $$grad(\\displaystyle\\sum_{k=1}^n(y_{pred} - y_{train})^2)$$ $$grad(\\displaystyle\\sum_{k=1}^n(W·x - y_{train})^2)$$\n",
    "5. Muevo todos los parámetros de $W$ hacia la dirección opuesta del gradiente: $$W -= learningRate * grad$$\n",
    "</font>"
   ],
   "id": "79ee4767f2b8dffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.weights_1 = np.random.randn(self.input_layer_size, self.hidden_layer_size) \n",
    "        self.weights_2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "      \n",
    "    @staticmethod\n",
    "    def loss_function(result: np.ndarray, expected: np.ndarray):\n",
    "        return np.mean((result - expected)**2)\n",
    "    \n",
    "    @staticmethod  \n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "            return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    def feedforward(self, input_data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "        h1 = np.dot(input_data, self.weights_1)\n",
    "        a1 = self.sigmoid(h1)\n",
    "        \n",
    "        logging.info(f\"Input data shape: {input_data.shape}\")\n",
    "        logging.info(f\"Weights hidden layer (W1): {self.weights_1.shape}\")\n",
    "        logging.info(f\"Activation hidden layer (a1): {a1.shape}\")\n",
    "        \n",
    "        h2 = np.dot(a1, self.weights_2)\n",
    "        a2 = self.sigmoid(h2)\n",
    "        \n",
    "        logging.info(f\"Weights output layer (W2): {self.weights_2.shape}\")\n",
    "        logging.info(f\"Activation output layer (a2): {a2.shape}\")\n",
    "        \n",
    "        return a2, a1\n",
    "        \n",
    "    def backpropagation(self, x_train: np.ndarray, y_train: np.ndarray, learning_rate):\n",
    "        if y_train.ndim == 1:\n",
    "            y_train = y_train.reshape(-1, 1)\n",
    "            \n",
    "        # Perform forward pass and get predictions and hidden layer activations\n",
    "        pred_output, hidden_activations = self.feedforward(input_data=x_train)\n",
    "        \n",
    "        # Compute output layer error and delta\n",
    "        output_error = y_train - pred_output\n",
    "        output_delta = output_error * self.sigmoid_derivative(pred_output)\n",
    "        \n",
    "        logging.info(f\"Output error shape: {output_error.shape}\")\n",
    "        logging.info(f\"Output derivative shape: {output_delta.shape}\")\n",
    "        \n",
    "        # Compute hidden layer error and delta\n",
    "        hidden_error = np.dot(output_delta, self.weights_2.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(hidden_activations)\n",
    "        \n",
    "        logging.info(f\"Hidden error shape: {hidden_error.shape}\")\n",
    "        logging.info(f\"Hidden derivative shape: {hidden_delta.shape}\")\n",
    "        \n",
    "        # Update weights\n",
    "        # Adjust weights between hidden and output layers\n",
    "        self.weights_2 += learning_rate * np.dot(hidden_activations.T, output_delta) / 1797\n",
    "        \n",
    "        # Adjust weights between input and hidden layers\n",
    "        self.weights_1 += learning_rate * np.dot(x_train.T, hidden_delta) / 1797\n",
    "        \n",
    "        # Compute and return total loss\n",
    "        return self.loss_function(pred_output, y_train)     \n",
    "    \n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs, learning_rate, print_every: int = 100):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backpropagation(x_train, y_train, learning_rate=learning_rate)\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")"
   ],
   "id": "f80dbbfb2cdf1468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nn = NeuralNetwork(input_layer_size=64, hidden_layer_size=64, output_layer_size=10)",
   "id": "2c3982921d782da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "digits = datasets.load_digits()\n",
    "training_data = digits.data # Images 8x8pixels -> 64 | 1797 examples -> ndarray (1797, 64)\n",
    "label_data = digits.target # Labels -> ndarray (1797, 1)"
   ],
   "id": "dbe05d94a0647cd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_train, x_test, y_train, y_test = train_test_split(training_data, label_data, test_size=0.2, random_state=42)",
   "id": "1ed09ba0695447a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nn.train(np.array(x_train), np.array(y_train), epochs=1000000, learning_rate=1, print_every=1000)",
   "id": "7702a2f4f4efae0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d21d1b304b51f277",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
