{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "7bb263e99aee1401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Backpropagation algorithm\n",
    "<font size=\"3\">\n",
    "Feedforward (inferencia):\n",
    "$$W · x = y_{pred}$$\n",
    "\n",
    "Backpropagation algorithm (entrenamiento):\n",
    "1. Inicializo los pesos ($W$) de manera aleatoria.\n",
    "2. Obtengo una primera predicción (pobre ya que los pesos son aleatorios). Cuanto de \"pobre\" lo calcula la función de pérdida:$$\\displaystyle\\sum_{k=1}^n(y_{pred} - y_{train})^2$$\n",
    "3. En el entrenamiento, como $x$ (la entrada) e $y_{train}$ (la salida) son fijas, solo se pueden modificar los pesos ($W$) para ajustar la red neuronal.\n",
    "4. Para ajustar $W$ calculamos el gradiente de la función de pérdida ya que el gradiente describe la direción que tomaria $y_{pred}$ al aumentar cualquiera de los valores de $W$: $$grad(\\displaystyle\\sum_{k=1}^n(y_{pred} - y_{train})^2)$$ $$grad(\\displaystyle\\sum_{k=1}^n(W·x - y_{train})^2)$$\n",
    "5. Muevo todos los parámetros de $W$ hacia la dirección opuesta del gradiente: $$W -= learningRate * grad$$\n",
    "</font>"
   ],
   "id": "79ee4767f2b8dffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.weights_1 = np.random.randn(self.input_layer_size, self.hidden_layer_size) * np.sqrt(2 / self.input_layer_size)\n",
    "        self.weights_2 = np.random.randn(self.hidden_layer_size, self.output_layer_size) * np.sqrt(2 / self.hidden_layer_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(result: np.ndarray, expected: np.ndarray):\n",
    "        return np.mean((result - expected)**2)\n",
    "    \n",
    "    @staticmethod  \n",
    "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "        return (x > 0).astype(float)\n",
    "        \n",
    "    def feedforward(self, input_data: np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "        h1 = np.dot(input_data, self.weights_1)\n",
    "        a1 = self.relu(h1)\n",
    "        \n",
    "        logging.info(f\"Input data shape: {input_data.shape}\")\n",
    "        logging.info(f\"Weights hidden layer (W1): {self.weights_1.shape}\")\n",
    "        logging.info(f\"Activation hidden layer (a1): {a1.shape}\")\n",
    "        \n",
    "        h2 = np.dot(a1, self.weights_2)\n",
    "        a2 = self.sigmoid(h2)\n",
    "        \n",
    "        logging.info(f\"Weights output layer (W2): {self.weights_2.shape}\")\n",
    "        logging.info(f\"Activation output layer (a2): {a2.shape}\")\n",
    "        \n",
    "        return a2, a1\n",
    "        \n",
    "    def backpropagation(self, x_train: np.ndarray, y_train: np.ndarray, learning_rate):\n",
    "        #y_train = y_train.reshape(-1, self.output_layer_size)\n",
    "\n",
    "        # Perform forward pass and get predictions and hidden layer activations\n",
    "        pred_output, hidden_activations = self.feedforward(input_data=x_train)\n",
    "        \n",
    "        # Compute output layer error and delta\n",
    "        output_error = y_train - pred_output\n",
    "        print(f\"y_train: {y_train}\")\n",
    "        print(f\"pred_output: {pred_output}\")\n",
    "        print(f\"output_error: {output_error}\")\n",
    "        \n",
    "        output_delta = output_error * self.sigmoid_derivative(pred_output)\n",
    "        \n",
    "        logging.info(f\"Output error shape: {output_error.shape}\")\n",
    "        logging.info(f\"Output derivative shape: {output_delta.shape}\")\n",
    "        \n",
    "        # Compute hidden layer error and delta\n",
    "        hidden_error = np.dot(output_delta, self.weights_2.T)\n",
    "        hidden_delta = hidden_error * self.relu_derivative(hidden_activations)\n",
    "        \n",
    "        logging.info(f\"Hidden error shape: {hidden_error.shape}\")\n",
    "        logging.info(f\"Hidden derivative shape: {hidden_delta.shape}\")\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights_2 += learning_rate * np.dot(hidden_activations.T, output_delta)\n",
    "        self.weights_1 += learning_rate * np.dot(x_train.T, hidden_delta)\n",
    "        \n",
    "        return self.loss_function(pred_output, y_train)     \n",
    "    \n",
    "    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs, learning_rate, print_every: int = 100):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backpropagation(x_train, y_train, learning_rate=learning_rate)\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "                \n",
    "    def predict(self, x_test):\n",
    "        predictions, _ = self.feedforward(x_test)\n",
    "        return predictions"
   ],
   "id": "f80dbbfb2cdf1468",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TRAINING",
   "id": "e88a70363cec6059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "digits = datasets.load_digits()\n",
    "training_data = digits.data # Images 8x8pixels -> 64 | 1797 examples -> ndarray (1797, 64)\n",
    "label_data = digits.target # Labels -> ndarray (1797, 1)"
   ],
   "id": "dbe05d94a0647cd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_train, x_test, y_train, y_test = train_test_split(training_data, label_data, test_size=0.2, random_state=9)",
   "id": "1ed09ba0695447a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train[0]",
   "id": "4d1d4fd4c6ddfd54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.matshow(x_train[0].reshape(8,8), cmap='grey')\n",
    "plt.show()"
   ],
   "id": "a9e042bf3282b95c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def encode_digit(digit: int) -> np.ndarray:\n",
    "    one_hot = np.zeros((10,))\n",
    "    one_hot[digit] = 1\n",
    "    return one_hot\n",
    "\n",
    "def encode_labels(labels_array: np.ndarray) -> np.ndarray:\n",
    "    encoded = [encode_digit(label) for label in labels_array]\n",
    "    return np.array(encoded)"
   ],
   "id": "c48a0442a5c332d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nn = NeuralNetwork(input_layer_size=64, hidden_layer_size=64, output_layer_size=10)",
   "id": "2c3982921d782da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train = encode_labels(labels_array=y_train)",
   "id": "d61a96b149c4b0a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nn.train(x_train, y_train, epochs=1000000, learning_rate=1, print_every=1000)",
   "id": "7702a2f4f4efae0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nn.feedforward(input_data=x_train[0].reshape(1, -1))[0]",
   "id": "56d18dfa8e8242ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train.shape",
   "id": "d202a0449147b372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x_train[0].reshape(1, -1)",
   "id": "7c8db094e96be9b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2f3d6489d039ca5f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
